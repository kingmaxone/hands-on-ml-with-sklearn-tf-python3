{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习之模型选择，训练与调优(四)\n",
    "\n",
    "上一篇文章对数据进行了预处理， 包括数据清洗，特征组合和类别数据编码，最后使用pipeline进行统一转换。  \n",
    "接下来开始选择模型对数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备工作\n",
    "#准备工作\n",
    "# 基本包的导入\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 画图相关\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 忽略警告\n",
    "import warnings\n",
    "\n",
    "# 图片存储目录\n",
    "PROJECT_ROOT_DIR = '../'\n",
    "CHAPTER_ID = 'end_to_end_project'\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"保存图片:\", fig_id)\n",
    "    \n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', module='scipy', message='internal')\n",
    "\n",
    "# 加载数据\n",
    "HOUSING_PATH = os.path.join(\"../datasets\", \"housing\")\n",
    "def loading_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = loading_housing_data()\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) \n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "        strat_train_set = housing.loc[train_index]\n",
    "        strat_test_set = housing.loc[test_index]\n",
    "\n",
    "for set in (strat_train_set, strat_test_set): \n",
    "    set.drop([\"income_cat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制一份训练集，并drop预测结果后语后续评估。\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述函数，其输入是包含1个多个枚举类别的2D数组，需要reshape成为这种数组\n",
    "# from sklearn.preprocessing import CategoricalEncoder  #后面会添加这个方法\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# 后面再去理解\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另外一个转换器：选择一个子集\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 如上，对于数据集需要进行大量的转换，并且要有一定的顺序。因此sklearn提供了pipeline来进行处理。\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "# 由于只能对数值类型进行处理，因此需要去除类别数据。\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attr_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n",
    "])\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 16)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运用结合pipeline对数值类型，类别类型同时进行转换\n",
    "housing_prepare = full_pipeline.fit_transform(housing)\n",
    "housing_prepare.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码就是前3篇文章的所有准备工作，得到housing_prepare就是需要用于算法的训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以对训练集 选择一个模型进行训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n",
      " 189747.55849879]\n",
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68628.19819848923"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性模型\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepare, housing_labels)\n",
    "\n",
    "# 训练完成，查看部分数据的训练结果：\n",
    "#利用管道对部分训练数据进行转换\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepare = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepare))  #预测数据\n",
    "# 与实际的真实值比较\n",
    "print(\"Labels:\", list(some_labels))\n",
    "\n",
    "# 对比以上预测，精确度不是很高。下面来计算一下此回归模型真实值与预测值之间的RMSE。\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepare)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上文提到，该值表示预测值与真实值之间的误差还不满足预测。这是一个欠拟合问题。  \n",
    "意味着训练集中的特征没有足够的信息来做预测；或者所选择模型不够强大。  \n",
    "针对这个问题，主要的解决方法有 选择更强大的模型，或者加入更多的特征。最后可以考虑对模型加入正则项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepare, housing_labels)\n",
    "\n",
    "housing_p = tree_reg.predict(housing_prepare)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_p)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse #没有误差，说明每个数据都预测正确，过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210644.60459286, 317768.80697211, 210956.43331178, ...,\n",
       "        95464.57062437, 214353.22541713, 276426.4692067 ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上说明模型过拟合了，完全没有误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用交叉验证做更好的评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种用于评估决策树模型的方式是将训练集划分成更小的训练集和一个验证集。    \n",
    "以下方法 使用K-fold 交叉验证，即将训练集随机划分为10个子集，将其中9个用作训练，一个用于验证。  \n",
    "进行10次训练， 结果是包含10次训练的评估分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score # 交叉验证\n",
    "scores = cross_val_score(tree_reg, housing_prepare, housing_labels, \n",
    "                       scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证期望得到的函数值越大越好，而不是像损失函数(代价函数)那样越小越好。  \n",
    "因此得分与MSE相反(结果为负)，因此需要使用以下方法计算rmse分数。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [69226.17632204 67694.79874797 70836.47151251 69944.84306415\n",
      " 71105.57199113 74536.65163847 69698.25274811 70014.21947239\n",
      " 76485.28736039 71113.75905971]\n",
      "Mean: 71065.60319168642\n",
      "Standard deviations: 2458.750570067908\n"
     ]
    }
   ],
   "source": [
    "tree_rmse_scores = np.sqrt(- scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviations:\", scores.std())\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于得到的均值比线性得到的分数还大，说明该模型还不如之前的线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于交叉验证不仅得到模型的性能估计，而且还可以得到精确度估计( 71000 +- 2500)。  \n",
    "如果只用一个验证集的话，是得不到这个信息的。但带来的时间损耗也是应该要考虑的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "Mean: 69052.46136345083\n",
      "Standard deviations: 2731.6740017983466\n"
     ]
    }
   ],
   "source": [
    "# 对线性回归使用交叉验证\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepare, housing_labels, \n",
    "                            scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比结果，决策树确实过拟合了，性能比现行回归模型要差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21899.2507207247"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用更强大的随机森林模型\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepare, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepare)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [51964.41327624 50609.19017883 52015.56310418 54915.30834399\n",
      " 51823.54440092 55504.33141367 51933.55364057 50394.88863904\n",
      " 55194.99947022 51783.09626233]\n",
      "Mean: 52613.888872999676\n",
      "Standard deviations: 1783.6375618811246\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepare, housing_labels, \n",
    "                               scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores) # 耗时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比结果，可以看到随机森林模型要好很多。不过需要注意的是， 交叉验证得到的分数还是比单独预测训练集的分数高太多(52000 >> 22000), 因此模型还是存在过拟合问题。  \n",
    "通常方法是简化模型(正则化)，或者加入更多的测试数据。  \n",
    "除以之外，还可以使用其他的模型，比如svm(使用不同的kernels)， 神经网络等。  \n",
    "这时候不必要花太多时间在调参上，目标是选出2-5个比较有希望的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 做个补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210644.60459286, 317768.80697211, 210956.43331178,  59218.98886849,\n",
       "       189747.55849879])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 完整的pipeline可以包括数据准备和预测\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "    (\"preparation\", full_pipeline), \n",
    "    (\"linear\", LinearRegression()),\n",
    "])\n",
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "full_pipeline_with_predictor.predict(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时需要对得到的模型进行持久化保存， 方便后续读取调优。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 持久化保存训练好的model：joblib\n",
    "my_model = full_pipeline_with_predictor\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "['my_model.pkl']\n",
    "# 读取\n",
    "my_model = joblib.load('my_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preparation', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(attribute_names=['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income'])), ('...ts=None)), ('linear', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中一种方法是手动调参，直到发现合适的超参数(hyperparameters), 这通常是是一个单调乏味的工作，并且还没有足够的时间去探索一些组合选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用 sklearn 的GridSearchCV 库 来进行网格搜索。 需要做的是给定哪些超参数以及要尝试的值，它会使用交叉验证来评估超参数值的所有可能组合。  \n",
    "如下对随机森林进行网格搜索。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    # 尝试 3*4=12中超参数组合\n",
    "    {'n_estimators':[3, 10,  30], 'max_features': [2, 4, 6, 8]},\n",
    "    # 尝试2*3=6种bootstrap\n",
    "    {'bootstrap': [False], 'n_estimators':[3, 10], 'max_features':[2, 3, 4]}\n",
    "]\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "# 如上所说，这里进行5次交叉验证。\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上，网格搜索针对随机森林需要的参数n_estimators， max_features，bootstrap组合进行穷举搜索。  \n",
    "总而言之，网格搜索将探索12 + 6 = 18的随机性的超参数值组合。  \n",
    "它将训练每个模型5次(因为我们使用的是5倍交叉验证)。  \n",
    "换句话说,总而言之,会有18×5 = 90轮培训!这可能需要很长时间，但当它完成时，你可以得到这样的参数的最佳组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于该步骤很耗时，因此为了演示，将训练数据减少一部分。\n",
    "housing_prepare_small = housing_prepare[:3000]\n",
    "housing_labels_small = housing_labels[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(housing_prepare_small, housing_labels_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 搜索结束\n",
    "grid_search.best_params_  #最好的超参数组合, 由于30是最大值，所以应该尝试更大的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=30, n_jobs=1, oob_score=False, random_state=42,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_  #可以直接得到 最优的模型用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71132.8432432034 {'max_features': 2, 'n_estimators': 3}\n",
      "59984.47727852012 {'max_features': 2, 'n_estimators': 10}\n",
      "56972.051178962836 {'max_features': 2, 'n_estimators': 30}\n",
      "65379.00225452585 {'max_features': 4, 'n_estimators': 3}\n",
      "57368.943967545856 {'max_features': 4, 'n_estimators': 10}\n",
      "54526.57612637526 {'max_features': 4, 'n_estimators': 30}\n",
      "62833.71167680876 {'max_features': 6, 'n_estimators': 3}\n",
      "56202.50853540545 {'max_features': 6, 'n_estimators': 10}\n",
      "54042.95646340654 {'max_features': 6, 'n_estimators': 30}\n",
      "63650.99752919532 {'max_features': 8, 'n_estimators': 3}\n",
      "56587.24638469238 {'max_features': 8, 'n_estimators': 10}\n",
      "54103.90905116381 {'max_features': 8, 'n_estimators': 30}\n",
      "67387.35090354811 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "58801.14857602752 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "64385.90097183371 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "57651.480112536025 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "62847.794784528916 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "55335.47191545115 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# 当然，可以得到每种组合的分数\n",
    "cvres = grid_search.cv_results_ \n",
    "\n",
    "for mean_scores, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "                               print(np.sqrt(-mean_scores), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里得到的最优模型 分数是6，30 -- 54042。对比之前默认的结果要好，因此这一步算是对模型进行的微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不要忘记，可以将一些预处理作为超参数处理。  \n",
    "例如，网格搜索将自动发现是否添加您不确定的特性(bedrooms_per_room超参数)。  \n",
    "同样，它也可以用来自动找到处理异常值的最佳方法、丢失的特征、特征选择等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当确定的超参数组合很少时，可以使用网格搜索。但是假如不知道超参数的范围或者其范围很大时，可以考虑使用随机搜索。  \n",
    "与网格搜索不同，它通过在每次迭代中为每个超参数选择一个随机值来评估给定数目的随机组合。有两个优点：  \n",
    "1. 指定随机搜索的次数， 探索不同的值，而不是像网格搜索针对对给定的值进行搜索。  \n",
    "2. 通过设定迭代次数，你可以对你想要分配给超参数搜索的计算有更多的控制。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_estimators\": randint(low=1, high=200),\n",
    "    \"max_features\": randint(low=1, high=8),\n",
    "}\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, \n",
    "                               n_iter=10, cv=5, scoring=\"neg_mean_squared_error\", random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a1cef1668>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a1cef1be0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.fit(housing_prepare_small, housing_labels_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53513.76883786206 {'max_features': 7, 'n_estimators': 180}\n",
      "55461.56488400293 {'max_features': 5, 'n_estimators': 15}\n",
      "54868.06214585608 {'max_features': 3, 'n_estimators': 72}\n",
      "55011.94093003847 {'max_features': 5, 'n_estimators': 21}\n",
      "53569.799862039145 {'max_features': 7, 'n_estimators': 122}\n",
      "54864.033006859674 {'max_features': 3, 'n_estimators': 75}\n",
      "54672.69350280115 {'max_features': 3, 'n_estimators': 88}\n",
      "53702.08699745629 {'max_features': 5, 'n_estimators': 100}\n",
      "54455.099383984176 {'max_features': 3, 'n_estimators': 150}\n",
      "72608.486575064 {'max_features': 5, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "# 如上进行了随机搜索， 结果展示如下：\n",
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得到随机搜索的最优值是7， 180。可以先利用随机搜索得到大概范围，再进行网格搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods 集成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微调系统的另一个方法是尝试将性能最好的模型进行组合。  \n",
    "组合模型(ensemble)一般要比最好的个体模型表现的要好。（就像随机森林比他们所依赖的个体决策树要好）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分析模型和误差\n",
    "通过对最好的模型进行检查，可以得到更有用的信息。比如随机森林回归可以指出每个属性的相对重要性，以便作出准确的预测.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06944296, 0.05731171, 0.04242502, 0.02077279, 0.01813785,\n",
       "       0.02092872, 0.0184342 , 0.32902876, 0.04881488, 0.10831886,\n",
       "       0.09421353, 0.01132378, 0.15255042, 0.        , 0.00313905,\n",
       "       0.00515749])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.32902875749672267, 'median_income'),\n",
       " (0.15255041700055916, 'INLAND'),\n",
       " (0.10831885804623398, 'pop_per_hhold'),\n",
       " (0.09421352604555039, 'bedrooms_per_room'),\n",
       " (0.06944295804995118, 'longitude'),\n",
       " (0.05731170766962609, 'latitude'),\n",
       " (0.048814879762369236, 'rooms_per_hhold'),\n",
       " (0.0424250223890338, 'housing_median_age'),\n",
       " (0.020928720432547652, 'population'),\n",
       " (0.020772789007452108, 'total_rooms'),\n",
       " (0.018434198917854132, 'households'),\n",
       " (0.01813785342906769, 'total_bedrooms'),\n",
       " (0.011323775759722297, '<1H OCEAN'),\n",
       " (0.005157489087866792, 'NEAR OCEAN'),\n",
       " (0.0031390469054428756, 'NEAR BAY'),\n",
       " (0.0, 'ISLAND')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat) \n",
    "\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过以上信息可以得到每个特征的重要性。因此可以在后面drop部分特征再进行训练。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在测试集上评估模型\n",
    "在调整过模型之后，是时候将最终的模型用于最初得到的测试数据上。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53813.45378519866"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse  # 目前所有值中的最低值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果进行了大量的超参数调优(因为模型在验证数据上执行得很好，并且很可能在未知的数据集上执行得很好), 那么性能通常会比您使用交叉验证所度量的要差一些。\n",
    "\n",
    "最后就是将模型用于实际的数据，并持续维护。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上四篇文章简单介绍了一个机器学习工程的主要步骤，包括但不限于：\n",
    "1. 获取数据并简单查看\n",
    "2. 基本统计数据并且可视化\n",
    "3. 数据清洗，特征组合与可视化\n",
    "4. 特征缩放， 编码，处理数据\n",
    "5. 优先选择不同转换器组合的 pipeline 来顺序处理数据，得到可预测的final_model模型。\n",
    "6. 选择模型训练，得到2-5个模型进行保存，后续调优。\n",
    "7. 针对不同模型， 进行交叉验证对比，使用随机搜索，网格搜索来得到更优的模型。\n",
    "8. 将最优模型用于测试，得到结果\n",
    "\n",
    "后续会简单介绍机器学习工程的大概步骤，相对而言会更详细一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
